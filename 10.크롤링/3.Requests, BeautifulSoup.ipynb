{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롤링 순서\n",
    "\n",
    "1. 원하는 웹페이지에 접속하여 HTML 데이터를 받아온다\n",
    "2. 받아온 HTML을 분석(파싱) 가능한 형태로 가공한다.\n",
    "3. 원하는 데이터를 추출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹페이지에 접속하기 위해 라이브러리 사용 urllib 는 기본 라이브러리\n",
    "import urllib.request\n",
    "\n",
    "# 웹서버에 request 하기 위한 객체를 생성해서 request 변수에 저장\n",
    "request = urllib.request.Request('https://www.naver.com')\n",
    "# request 를 통해 urlopen 후 결과를 response 변수에 저장\n",
    "response = urllib.request.urlopen(request)\n",
    "# response 된 내용을 read()\n",
    "htmlBytes = response.read()\n",
    "# htmlBytes 는 bytes 형태로 저장됨 따라서 문자열로 사용하기 위해 utf-8로 디코딩\n",
    "htmlStr = htmlBytes.decode(\"utf8\")\n",
    "# htmlStr 의 내용을 엔터값으로 분리 (보기좋게 하기 위해)\n",
    "htmlSplit = htmlStr.split('\\n')\n",
    "# 분리된 내용을 줄단위로 출력\n",
    "for line in htmlSplit:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requests\n",
    "\n",
    "* 파이썬에서 웹서버에 접속해서 HTML을 받아올 수 있는 외부 라이브러리 입니다.\n",
    "* 파이썬 설치시 설치되는 기본라이브러리 보다 사용성도 편하고 속도도 더 빠릅니다.\n",
    "* ***pip install requests*** 로 설치 후 사용할 수 있습니다.\n",
    "* <a href=\"https://2.python-requests.org/en/master/\">Requests 공식사이트</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 메인 페이지 HTML 가져오기\n",
    "import requests\n",
    "response = requests.get(\"https://www.naver.com\")\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.headers['content-type'])\n",
    "print(response.encoding)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup\n",
    "\n",
    "* HTML을 분석 가능한 형태로 가공하고 추출할 수 있는 외부 라이브러리 입니다.\n",
    "* ***pip install BeautifulSoup4*** 로 설치 후 사용할 수 있습니다.\n",
    "* BeautifulSoup 사용시 해석라이브러리(파서)를 지정해줘야 합니다.\n",
    "* 파서로는 기본적인 html.parser, lxml 등이 있습니다. lxml 은 외부라이브러리로 따로 설치를 해야만 사용할 수 있습니다.\n",
    "\n",
    "* <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">BeautifulSoup 공식 사이트</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 메인 페이지에서 A 링크만 추출하기\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get(\"https://www.naver.com\")\n",
    "bs = BeautifulSoup(response.text, \"html.parser\")\n",
    "print(bs.select(\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requests-HTML\n",
    "\n",
    "* requests 제작자가 만든 requests에 기능을 추가하여 확장시킨 외부 라이브러리 입니다.\n",
    "* requests_html 내부에서 requests를 사용합니다.\n",
    "* requests 에서 지원하지 않는 javasript을 지원하여 javascript를 렌더링 할 수 있습니다.\n",
    "* ***pip install requests_html*** 로 설치 후 사용할 수 있습니다.\n",
    "* <a href=\"https://html.python-requests.org/\">requests_html 공식사이트</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_html import HTMLSession\n",
    "\n",
    "session = HTMLSession()\n",
    "r = session.get('https://www.naver.com')\n",
    "print(r.html.links)\n",
    "print(r.html.absolute_links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
